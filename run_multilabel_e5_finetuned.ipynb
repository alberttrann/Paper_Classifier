{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-24T16:53:01.924334Z",
          "iopub.status.busy": "2025-08-24T16:53:01.923612Z",
          "iopub.status.idle": "2025-08-24T16:53:05.47175Z",
          "shell.execute_reply": "2025-08-24T16:53:05.470733Z",
          "shell.execute_reply.started": "2025-08-24T16:53:01.924314Z"
        },
        "id": "VvER0l6dMEia",
        "outputId": "bf0cadd1-6a6a-4a4d-9afd-b31db9c097b0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers accelerate datasets nltk scikit-learn torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8fda18a57f6e434f846731790775bff3",
            "deb4c790f3c84cd3bf1465a157286536",
            "5abd82a521db418ab062713d0fabee98",
            "44b9d7dc5cbc48668be882a84b5289ae",
            "d7810a1c3cff4f39a759fdbd712331c4",
            "a0a65ef0df9940ab92be8e0cd23811a8",
            "617207318b154e57b89c86f1480a547b",
            "cb1bb9347959496c830518bc3add2e4d",
            "5a87301e0aa34f238f2de0352b74c529",
            "b15bf4272feb4db1a333f4ab14a05168",
            "74abf964ceca4ac98e7f8e334bef46be",
            "b8625cd16e3e4eba8d69a05725503461",
            "930099afb8dd4542897a4ae97e7fdfdf",
            "1cfaebcea60d49a5b5faed5ab41bfb05"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-08-24T16:53:27.036879Z",
          "iopub.status.busy": "2025-08-24T16:53:27.036566Z",
          "iopub.status.idle": "2025-08-24T22:13:07.521949Z",
          "shell.execute_reply": "2025-08-24T22:13:07.52114Z",
          "shell.execute_reply.started": "2025-08-24T16:53:27.036816Z"
        },
        "id": "mt4VK2rNMEif",
        "outputId": "4a82716f-d647-493b-b246-327e5bd696cd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "================================================================================\n",
            "--- Deep Learning Multi-Label Benchmark: 2025-08-24 16:53:38 ---\n",
            "================================================================================\n",
            "--- Step 1: Multi-Label Data Sampling & Preparation ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fda18a57f6e434f846731790775bff3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/810 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deb4c790f3c84cd3bf1465a157286536",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Scanning for samples: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished sampling. Total samples collected: 77590\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5abd82a521db418ab062713d0fabee98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Cleaning Abstracts:   0%|          | 0/77590 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Step 2: Tokenizing Text and Creating PyTorch Datasets ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44b9d7dc5cbc48668be882a84b5289ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7810a1c3cff4f39a759fdbd712331c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0a65ef0df9940ab92be8e0cd23811a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "617207318b154e57b89c86f1480a547b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Step 3: Initializing Model, Loss Function, and Optimizer ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb1bb9347959496c830518bc3add2e4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-24 16:55:28.750433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756054528.935326     148 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756054528.989157     148 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a87301e0aa34f238f2de0352b74c529",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Step 4: Starting Fine-Tuning Loop ---\n",
            "\n",
            "--- Epoch 1/4 ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b15bf4272feb4db1a333f4ab14a05168",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/7759 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train loss: 0.1324\n",
            "\n",
            "--- Epoch 2/4 ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74abf964ceca4ac98e7f8e334bef46be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/7759 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train loss: 0.0954\n",
            "\n",
            "--- Epoch 3/4 ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8625cd16e3e4eba8d69a05725503461",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/7759 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train loss: 0.0795\n",
            "\n",
            "--- Epoch 4/4 ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "930099afb8dd4542897a4ae97e7fdfdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/7759 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train loss: 0.0689\n",
            "\n",
            "--- Step 5: Final Evaluation on Test Set ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cfaebcea60d49a5b5faed5ab41bfb05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/1940 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Model: Fine-Tuned Transformer (intfloat/multilingual-e5-base)\n",
            "==================================================\n",
            "Overall Subset Accuracy: 0.7962\n",
            "Hamming Loss: 0.0370\n",
            "\n",
            "Per-Category Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        math       0.93      0.91      0.92      4441\n",
            "    astro-ph       0.97      0.92      0.95      3378\n",
            "          cs       0.86      0.78      0.82       950\n",
            "    cond-mat       0.91      0.87      0.89      3213\n",
            "     physics       0.76      0.57      0.65      1365\n",
            "      hep-ph       0.87      0.83      0.85      1543\n",
            "    quant-ph       0.70      0.84      0.77      1102\n",
            "      hep-th       0.78      0.81      0.79      1564\n",
            "\n",
            "   micro avg       0.88      0.85      0.87     17556\n",
            "   macro avg       0.85      0.82      0.83     17556\n",
            "weighted avg       0.88      0.85      0.87     17556\n",
            " samples avg       0.89      0.89      0.88     17556\n",
            "\n",
            "\n",
            "Deep learning benchmark complete. Results appended to 'deep_learning_multilabel.txt'.\n"
          ]
        }
      ],
      "source": [
        "# run_deep_learning_multilabel.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "\n",
        "# NLTK for text cleaning\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# --- NEW: PyTorch and Transformers Imports ---\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Scikit-learn for metrics and data splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, hamming_loss\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Data Sampling\n",
        "CATEGORIES_TO_SELECT = [\n",
        "    'math', 'astro-ph', 'cs', 'cond-mat', 'physics',\n",
        "    'hep-ph', 'quant-ph', 'hep-th'\n",
        "]\n",
        "SAMPLES_PER_CATEGORY_APPEARANCE = 5000\n",
        "\n",
        "# Model & Training\n",
        "E5_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
        "RANDOM_STATE = 42\n",
        "BATCH_SIZE = 8 # Reduced batch size to mitigate OOM error\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5 # A standard learning rate for fine-tuning\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LOG_FILE_PATH = \"deep_learning_multilabel.txt\"\n",
        "\n",
        "# --- NLTK Downloads (run once if not already downloaded) ---\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    word_tokenize(\"test\")\n",
        "except LookupError:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    WordNetLemmatizer().lemmatize(\"test\")\n",
        "except LookupError:\n",
        "    import nltk\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.data.find('tokenizers/punkt_tab/english/')\n",
        "except LookupError:\n",
        "    import nltk\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# --- Helper function for logging ---\n",
        "def log_message(message, to_console=True):\n",
        "    if to_console:\n",
        "        print(message)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(message + '\\n')\n",
        "\n",
        "# --- Enhanced Text Preprocessing Function ---\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "domain_specific_stopwords = {'result', 'study', 'show', 'paper', 'model', 'analysis', 'method', 'approach', 'propose', 'demonstrate', 'investigate'}\n",
        "stop_words.update(domain_specific_stopwords)\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "# --- NEW: PyTorch Dataset Class ---\n",
        "class ArxivMultiLabelDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item_idx):\n",
        "        text = self.texts[item_idx]\n",
        "        label = self.labels[item_idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "# --- NEW: Custom Transformer Model for Multi-Label Classification ---\n",
        "class MultiLabelTransformer(torch.nn.Module):\n",
        "    def __init__(self, base_model_name, n_classes):\n",
        "        super(MultiLabelTransformer, self).__init__()\n",
        "        self.transformer = AutoModel.from_pretrained(base_model_name)\n",
        "        # Add a dropout layer for regularization\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "        # The final linear layer that maps the embedding to our 8 classes\n",
        "        self.classifier = torch.nn.Linear(self.transformer.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get the embeddings from the base transformer\n",
        "        transformer_output = self.transformer(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # Use the embedding of the [CLS] token for classification\n",
        "        pooled_output = transformer_output.pooler_output\n",
        "\n",
        "        # Apply dropout and the final classification layer\n",
        "        output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(output)\n",
        "        return logits\n",
        "\n",
        "# --- NEW: Training and Evaluation Functions ---\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = loss_fn(logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def eval_model(model, data_loader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            # Apply sigmoid to get probabilities, then apply threshold\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > threshold).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# --- Main Execution ---\n",
        "log_message(\"\\n\\n\" + \"=\"*80)\n",
        "log_message(f\"--- Deep Learning Multi-Label Benchmark: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
        "log_message(\"=\"*80)\n",
        "\n",
        "# 1. Multi-Label Data Sampling and Preparation\n",
        "print(\"--- Step 1: Multi-Label Data Sampling & Preparation ---\")\n",
        "category_counts = {cat: 0 for cat in CATEGORIES_TO_SELECT}\n",
        "samples = []\n",
        "dataset_generator = load_dataset(\"UniverseTBD/arxiv-abstracts-large\", split=\"train\", streaming=True)\n",
        "for s in tqdm(dataset_generator, desc=\"Scanning for samples\"):\n",
        "    if all(count >= SAMPLES_PER_CATEGORY_APPEARANCE for count in category_counts.values()):\n",
        "        break\n",
        "    if s['categories'] is None or s['abstract'] is None: continue\n",
        "    parent_categories = {cat.split('.')[0] for cat in s['categories'].strip().split(' ')}\n",
        "    if any(p in CATEGORIES_TO_SELECT for p in parent_categories):\n",
        "        samples.append({'abstract': s['abstract'], 'parent_categories': parent_categories})\n",
        "        for p_cat in parent_categories:\n",
        "            if p_cat in category_counts:\n",
        "                category_counts[p_cat] += 1\n",
        "print(f\"Finished sampling. Total samples collected: {len(samples)}\")\n",
        "abstracts = [sample['abstract'] for sample in samples]\n",
        "labels_sets = [sample['parent_categories'] for sample in samples]\n",
        "processed_abstracts = [clean_text(abstract) for abstract in tqdm(abstracts, desc=\"Cleaning Abstracts\")]\n",
        "Y = np.zeros((len(samples), len(CATEGORIES_TO_SELECT)), dtype=int)\n",
        "cat_to_idx = {cat: i for i, cat in enumerate(CATEGORIES_TO_SELECT)}\n",
        "for i, label_set in enumerate(labels_sets):\n",
        "    for label in label_set:\n",
        "        if label in cat_to_idx:\n",
        "            Y[i, cat_to_idx[label]] = 1\n",
        "\n",
        "train_texts, test_texts, Y_train, Y_test = train_test_split(\n",
        "    processed_abstracts, Y, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# 2. Tokenization and Dataset Creation\n",
        "print(\"\\n--- Step 2: Tokenizing Text and Creating PyTorch Datasets ---\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(E5_MODEL_NAME)\n",
        "train_dataset = ArxivMultiLabelDataset(train_texts, Y_t rain, tokenizer)\n",
        "test_dataset = ArxivMultiLabelDataset(test_texts, Y_test, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 3. Model Initialization\n",
        "print(\"\\n--- Step 3: Initializing Model, Loss Function, and Optimizer ---\")\n",
        "model = MultiLabelTransformer(E5_MODEL_NAME, n_classes=len(CATEGORIES_TO_SELECT))\n",
        "model = model.to(DEVICE)\n",
        "# Use BCEWithLogitsLoss for multi-label classification\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 4. Training Loop\n",
        "print(\"\\n--- Step 4: Starting Fine-Tuning Loop ---\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
        "    train_loss = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
        "    print(f\"  Train loss: {train_loss:.4f}\")\n",
        "\n",
        "# 5. Final Evaluation\n",
        "print(\"\\n--- Step 5: Final Evaluation on Test Set ---\")\n",
        "Y_pred, Y_true = eval_model(model, test_loader, DEVICE)\n",
        "\n",
        "# Log results\n",
        "accuracy = accuracy_score(Y_true, Y_pred)\n",
        "hamming = hamming_loss(Y_true, Y_pred)\n",
        "report = classification_report(Y_true, Y_pred, target_names=CATEGORIES_TO_SELECT, zero_division=0)\n",
        "\n",
        "log_message(\"\\n\" + \"=\"*50 + f\"\\nModel: Fine-Tuned Transformer ({E5_MODEL_NAME})\\n\" + \"=\"*50)\n",
        "log_message(f\"Overall Subset Accuracy: {accuracy:.4f}\")\n",
        "log_message(f\"Hamming Loss: {hamming:.4f}\\n\")\n",
        "log_message(\"Per-Category Performance:\")\n",
        "log_message(report)\n",
        "\n",
        "print(f\"\\nDeep learning benchmark complete. Results appended to '{LOG_FILE_PATH}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-24T22:18:37.214726Z",
          "iopub.status.busy": "2025-08-24T22:18:37.214121Z",
          "iopub.status.idle": "2025-08-24T22:18:40.064493Z",
          "shell.execute_reply": "2025-08-24T22:18:40.063882Z",
          "shell.execute_reply.started": "2025-08-24T22:18:37.214699Z"
        },
        "id": "g4FA8bORMEij",
        "outputId": "6b279f1d-b492-4052-ff5e-84737fdaa564",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to save the fine-tuned components from the current session...\n",
            "Successfully saved model components and tokenizer to ./e5_finetuned_multilabel\n",
            "You can now find this folder in the file browser and download it.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "output_model_dir = \"./e5_finetuned_multilabel\"\n",
        "os.makedirs(output_model_dir, exist_ok=True)\n",
        "\n",
        "print(\"Attempting to save the fine-tuned components from the current session...\")\n",
        "try:\n",
        "    # We save the underlying transformer model, which has the save_pretrained method.\n",
        "    model.transformer.save_pretrained(output_model_dir)\n",
        "\n",
        "    # We also need to save the final classification layer's weights manually\n",
        "    torch.save(model.classifier.state_dict(), os.path.join(output_model_dir, \"classifier_weights.bin\"))\n",
        "\n",
        "    # The tokenizer is saved as before\n",
        "    tokenizer.save_pretrained(output_model_dir)\n",
        "\n",
        "    print(f\"Successfully saved model components and tokenizer to {output_model_dir}\")\n",
        "    print(\"You can now find this folder in the file browser and download it.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: The 'model' or 'tokenizer' variable was not found in the current session.\")\n",
        "    print(\"This can happen if the kernel was restarted or the training script did not complete successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Binary Classification with Neural Networks",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 12937777,
          "sourceId": 91719,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
