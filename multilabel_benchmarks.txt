

================================================================================
--- Multi-Label Championship Benchmark: 2025-08-24 09:56:23 ---
================================================================================


--- Tier 1: Binary Relevance with LR(tfidf) ---
Overall Subset Accuracy: 0.5624
Hamming Loss: 0.0747

Per-Category Performance:
              precision    recall  f1-score   support

        math       0.71      0.88      0.79      1354
    astro-ph       0.84      0.92      0.88      1159
          cs       0.76      0.91      0.83      1059
    cond-mat       0.70      0.87      0.78      1233
     physics       0.51      0.84      0.64      1011
      hep-ph       0.75      0.89      0.81      1001
    quant-ph       0.71      0.88      0.78       950
      hep-th       0.70      0.88      0.78      1055

   micro avg       0.70      0.88      0.78      8822
   macro avg       0.71      0.88      0.79      8822
weighted avg       0.71      0.88      0.79      8822
 samples avg       0.77      0.91      0.81      8822



--- Tier 2: Binary Relevance with Soft Voting Ensembles ---
Overall Subset Accuracy: 0.6895
Hamming Loss: 0.0518

Per-Category Performance:
              precision    recall  f1-score   support

        math       0.87      0.77      0.82      1354
    astro-ph       0.94      0.86      0.90      1159
          cs       0.90      0.84      0.87      1059
    cond-mat       0.89      0.72      0.79      1233
     physics       0.86      0.48      0.62      1011
      hep-ph       0.91      0.80      0.85      1001
    quant-ph       0.90      0.75      0.82       950
      hep-th       0.86      0.76      0.80      1055

   micro avg       0.89      0.75      0.82      8822
   macro avg       0.89      0.75      0.81      8822
weighted avg       0.89      0.75      0.81      8822
 samples avg       0.81      0.79      0.79      8822



--- Tier 3: Binary Relevance with Stacking Ensembles ---
Overall Subset Accuracy: 0.6362
Hamming Loss: 0.0606

Per-Category Performance:
              precision    recall  f1-score   support

        math       0.77      0.87      0.82      1354
    astro-ph       0.87      0.94      0.91      1159
          cs       0.82      0.91      0.86      1059
    cond-mat       0.74      0.88      0.80      1233
     physics       0.60      0.85      0.70      1011
      hep-ph       0.79      0.92      0.85      1001
    quant-ph       0.76      0.90      0.82       950
      hep-th       0.71      0.90      0.79      1055

   micro avg       0.75      0.89      0.82      8822
   macro avg       0.76      0.90      0.82      8822
weighted avg       0.76      0.89      0.82      8822
 samples avg       0.82      0.92      0.84      8822



--- Multi-Label Championship Summary ---
Model Architecture                            | Subset Accuracy      | Hamming Loss (Lower is Better)
-----------------------------------------------------------------------------------------------------
Tier 1: Binary Relevance with LR(tfidf)       | 0.5624               | 0.0747                        
Tier 2: Binary Relevance with Soft Voting     | 0.6895               | 0.0518                        
Tier 3: Binary Relevance with Stacking        | 0.6362               | 0.0606                        
-----------------------------------------------------------------------------------------------------
